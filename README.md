# Multi-Layered-Perceptrons
MLP on Hand Written digit

A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to refer to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation). Multilayer perceptrons are sometimes colloquially referred to as "vanilla" neural networks, especially when they have a single hidden layer.

An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.

Dropout is a regularization technique for neural network models.A Simple Way to Prevent Neural Networks from Overfitting.Dropout is a technique where randomly selected neurons are ignored during training.

Batch normalization : To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation,so that each batch will be having same kind of distribution even at deaper layer of Nueral Network.

In this IPYNB i have trained different MLP with different number of hidden layer ,droupouts and batch normilization .we can see how the result varies with the change in the architecture of Neural Network. 

# Facing error while loading IPYNB "Sorry, something went wrong. Reload?"
please click the following link https://nbviewer.jupyter.org/github/prassena/Multi-Layered-Perceptrons/blob/master/Multi%20Layered%20Perceptrons.ipynb
